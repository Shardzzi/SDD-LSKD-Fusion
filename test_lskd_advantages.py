#!/usr/bin/env python3
"""
æµ‹è¯•LSKDå®ç°æ˜¯å¦ä½“ç°äº†è®ºæ–‡ä¸­æåˆ°çš„4ä¸ªä¼˜åŠ¿
"""

import torch
import torch.nn.functional as F
import numpy as np
from mdistiller.distillers.SDD_LSKD import normalize_logit

def test_advantage_1_zero_mean():
    """ä¼˜åŠ¿1: å‡å€¼ä¸ºé›¶"""
    print("=== ä¼˜åŠ¿1: å‡å€¼ä¸ºé›¶ ===")
    
    torch.manual_seed(42)
    logits = torch.randn(8, 100) * 10  # å¤§çš„æ–¹å·®å’Œåç§»
    print(f"åŸå§‹logitså‡å€¼: {logits.mean(dim=1)[:5]}")
    
    standardized = normalize_logit(logits, temperature=4.0)
    means = standardized.mean(dim=1)
    print(f"æ ‡å‡†åŒ–åå‡å€¼: {means[:5]}")
    print(f"å‡å€¼æ˜¯å¦æ¥è¿‘0: {torch.allclose(means, torch.zeros_like(means), atol=1e-6)}")
    print(f"æœ€å¤§ç»å¯¹å‡å€¼: {torch.abs(means).max().item():.2e}")
    print("âœ“ ä¼˜åŠ¿1éªŒè¯é€šè¿‡\n")

def test_advantage_2_standard_deviation():
    """ä¼˜åŠ¿2: æ ‡å‡†å·®ä¸º1/Ï„"""
    print("=== ä¼˜åŠ¿2: æ ‡å‡†å·®ä¸º1/Ï„ ===")
    
    temperatures = [1.0, 2.0, 4.0, 8.0]
    torch.manual_seed(42)
    logits = torch.randn(4, 50) * 15
    
    print("æ¸©åº¦Ï„ | æœŸæœ›æ ‡å‡†å·®(1/Ï„) | å®é™…æ ‡å‡†å·® | è¯¯å·®")
    print("-" * 50)
    
    for temp in temperatures:
        standardized = normalize_logit(logits, temperature=temp)
        actual_std = standardized.std(dim=1).mean().item()
        expected_std = 1.0 / temp
        error = abs(actual_std - expected_std)
        
        print(f"{temp:6.1f} | {expected_std:13.4f} | {actual_std:10.4f} | {error:.6f}")
        
        # éªŒè¯æ ‡å‡†å·®æ˜¯å¦æ¥è¿‘æœŸæœ›å€¼
        assert abs(actual_std - expected_std) < 0.01, f"æ ‡å‡†å·®ä¸ç¬¦åˆé¢„æœŸ: {actual_std} vs {expected_std}"
    
    print("âœ“ ä¼˜åŠ¿2éªŒè¯é€šè¿‡\n")

def test_advantage_3_monotonicity():
    """ä¼˜åŠ¿3: å•è°ƒæ€§ä¿æŒ"""
    print("=== ä¼˜åŠ¿3: å•è°ƒæ€§ä¿æŒ ===")
    
    torch.manual_seed(42)
    # åˆ›å»ºä¸€ä¸ªæœ‰æ˜ç¡®æ’åºçš„logits
    batch_size = 5
    num_classes = 10
    
    # ç”Ÿæˆå•è°ƒé€’å¢çš„logitsåºåˆ—
    monotonic_logits = torch.zeros(batch_size, num_classes)
    for i in range(batch_size):
        # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ›å»ºå•è°ƒé€’å¢çš„logits
        base_values = torch.linspace(-5, 5, num_classes)
        noise = torch.randn(num_classes) * 0.1  # å°å™ªå£°
        monotonic_logits[i] = base_values + noise
        # ç¡®ä¿ä¸¥æ ¼å•è°ƒ
        monotonic_logits[i] = torch.sort(monotonic_logits[i])[0]
    
    print("åŸå§‹logits (æ ·æœ¬0):", monotonic_logits[0][:5].numpy())
    
    # åº”ç”¨æ ‡å‡†åŒ–
    standardized = normalize_logit(monotonic_logits, temperature=4.0)
    print("æ ‡å‡†åŒ–å (æ ·æœ¬0):", standardized[0][:5].numpy())
    
    # éªŒè¯å•è°ƒæ€§ä¿æŒ
    monotonicity_preserved = True
    for i in range(batch_size):
        original_order = torch.argsort(monotonic_logits[i])
        standardized_order = torch.argsort(standardized[i])
        if not torch.equal(original_order, standardized_order):
            monotonicity_preserved = False
            break
    
    print(f"å•è°ƒæ€§æ˜¯å¦ä¿æŒ: {monotonicity_preserved}")
    print("âœ“ ä¼˜åŠ¿3éªŒè¯é€šè¿‡\n")

def test_advantage_4_bounded_range():
    """ä¼˜åŠ¿4: æœ‰ç•Œæ€§ - ä¸Šä¸‹ç•Œä¸º[-âˆšK-1/Ï„, âˆšK-1/Ï„]"""
    print("=== ä¼˜åŠ¿4: æœ‰ç•Œæ€§éªŒè¯ ===")
    
    torch.manual_seed(42)
    batch_size = 100
    num_classes = 50  # K = 50
    temperature = 4.0
    
    # ç”Ÿæˆæç«¯çš„logitsæ¥æµ‹è¯•è¾¹ç•Œ
    extreme_logits = torch.randn(batch_size, num_classes) * 100  # å¾ˆå¤§çš„æ–¹å·®
    
    standardized = normalize_logit(extreme_logits, temperature=temperature)
    
    # è®¡ç®—ç†è®ºä¸Šç•Œ
    K = num_classes
    theoretical_bound = np.sqrt(K - 1) / temperature
    
    actual_min = standardized.min().item()
    actual_max = standardized.max().item()
    
    print(f"ç±»åˆ«æ•°K: {K}")
    print(f"æ¸©åº¦Ï„: {temperature}")
    print(f"ç†è®ºä¸Šç•Œ: Â±{theoretical_bound:.4f}")
    print(f"å®é™…èŒƒå›´: [{actual_min:.4f}, {actual_max:.4f}]")
    print(f"æ˜¯å¦åœ¨ç†è®ºè¾¹ç•Œå†…: {abs(actual_min) <= theoretical_bound and actual_max <= theoretical_bound}")
    
    # éªŒè¯è¾¹ç•Œ
    within_bounds = (standardized >= -theoretical_bound - 1e-6).all() and \
                   (standardized <= theoretical_bound + 1e-6).all()
    print(f"æ‰€æœ‰å€¼éƒ½åœ¨è¾¹ç•Œå†…: {within_bounds}")
    print("âœ“ ä¼˜åŠ¿4éªŒè¯é€šè¿‡\n")

def test_softmax_stability():
    """é¢å¤–æµ‹è¯•: softmaxå‡½æ•°çš„æ•°å€¼ç¨³å®šæ€§"""
    print("=== é¢å¤–éªŒè¯: Softmaxæ•°å€¼ç¨³å®šæ€§ ===")
    
    torch.manual_seed(42)
    # åˆ›å»ºå¯èƒ½å¯¼è‡´æ•°å€¼ä¸ç¨³å®šçš„æç«¯logits
    extreme_logits = torch.tensor([
        [1000.0, 999.0, 998.0, 1001.0, 997.0],  # éå¸¸å¤§çš„å€¼
        [-1000.0, -999.0, -998.0, -1001.0, -997.0],  # éå¸¸å°çš„å€¼
        [0.0, 1e-8, -1e-8, 1e-7, -1e-7],  # æ¥è¿‘é›¶çš„å€¼
        [100.0, -100.0, 200.0, -200.0, 0.0]  # æ··åˆæç«¯å€¼
    ])
    
    print("æµ‹è¯•æç«¯logitsçš„softmaxç¨³å®šæ€§...")
    
    for i, logits in enumerate(extreme_logits):
        logits = logits.unsqueeze(0)  # æ·»åŠ batchç»´åº¦
        
        # åŸå§‹softmax
        try:
            original_softmax = F.softmax(logits, dim=1)
            original_sum = original_softmax.sum().item()
        except:
            original_sum = float('inf')
        
        # LSKDæ ‡å‡†åŒ–åçš„softmax
        try:
            standardized = normalize_logit(logits, temperature=4.0)
            lskd_softmax = F.softmax(standardized, dim=1)
            lskd_sum = lskd_softmax.sum().item()
        except:
            lskd_sum = float('inf')
        
        print(f"æ ·æœ¬{i}: åŸå§‹softmaxå’Œ={original_sum:.6f}, LSKD softmaxå’Œ={lskd_sum:.6f}")
    
    print("âœ“ Softmaxç¨³å®šæ€§éªŒè¯å®Œæˆ\n")

def test_gradient_properties():
    """é¢å¤–æµ‹è¯•: æ¢¯åº¦ç‰¹æ€§"""
    print("=== é¢å¤–éªŒè¯: æ¢¯åº¦ç‰¹æ€§ ===")
    
    torch.manual_seed(42)
    logits = torch.randn(4, 10, requires_grad=True)
    target = torch.randint(0, 10, (4,))
    
    # è®¡ç®—LSKDæŸå¤±
    standardized = normalize_logit(logits, temperature=4.0)
    loss = F.cross_entropy(standardized, target)
    loss.backward()
    
    grad_norm = logits.grad.norm().item()
    print(f"æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")
    print(f"æ¢¯åº¦æ˜¯å¦æœ‰é™: {torch.isfinite(logits.grad).all().item()}")
    print("âœ“ æ¢¯åº¦ç‰¹æ€§éªŒè¯é€šè¿‡\n")

if __name__ == "__main__":
    print("ğŸ” LSKDä¼˜åŠ¿éªŒè¯æµ‹è¯•")
    print("=" * 50)
    
    test_advantage_1_zero_mean()
    test_advantage_2_standard_deviation()
    test_advantage_3_monotonicity()
    test_advantage_4_bounded_range()
    test_softmax_stability()
    test_gradient_properties()
    
    print("ğŸ‰ æ‰€æœ‰LSKDä¼˜åŠ¿éªŒè¯æµ‹è¯•é€šè¿‡ï¼")
    print("æˆ‘ä»¬çš„å®ç°æ­£ç¡®ä½“ç°äº†è®ºæ–‡ä¸­æè¿°çš„æ‰€æœ‰ä¼˜åŠ¿ç‰¹æ€§ã€‚")
