Start time: Wed Jun 11 22:43:21 CST 2025
Configuration: configs/cifar100/sdd_dkd_lskd/res32x4_shuv1.yaml
M setting: [1]
=========================================
<class 'str'>
[36m[INFO] CONFIG:
DATASET:
  NUM_WORKERS: 2
  TEST:
    BATCH_SIZE: 64
  TYPE: cifar100
DISTILLER:
  STUDENT: ShuffleV1_sdd
  TEACHER: resnet32x4_sdd
  TYPE: SDD_DKD_LSKD
EXPERIMENT:
  NAME: ''
  PROJECT: cifar100_baselines
  TAG: sdd_dkd_lskd,res32x4,shuv1
LOG:
  PREFIX: ./output
  SAVE_CHECKPOINT_FREQ: 40
  TENSORBOARD_FREQ: 500
  WANDB: false
SOLVER:
  BATCH_SIZE: 64
  EPOCHS: 240
  LR: 0.05
  LR_DECAY_RATE: 0.1
  LR_DECAY_STAGES:
  - 150
  - 180
  - 210
  MOMENTUM: 0.9
  TRAINER: base
  TYPE: SGD
  WEIGHT_DECAY: 0.0005
[0m
Files already downloaded and verified
Files already downloaded and verified
[36m[INFO] Loading teacher model[0m
[1]
[1]
[36m[INFO] Extra parameters of SDD_DKD_LSKD: 0[0m[0m
Epoch 1: Training...
/root/repos/SDD-LSKD-Fusion/mdistiller/engine/utils.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(f, map_location="cpu")
/root/miniconda3/envs/sdd-lskd-fusion/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
  Batch 100/782 - Loss CE: 9.2603, Loss KD: 2.8854
  Batch 200/782 - Loss CE: 6.8355, Loss KD: 2.6844
  Batch 300/782 - Loss CE: 5.9356, Loss KD: 2.5804
  Batch 400/782 - Loss CE: 5.4399, Loss KD: 2.5119
  Batch 500/782 - Loss CE: 5.1324, Loss KD: 2.4663
  Batch 600/782 - Loss CE: 4.9002, Loss KD: 2.4275
  Batch 700/782 - Loss CE: 4.7291, Loss KD: 2.3960
  Batch 782/782 - Loss CE: 4.6089, Loss KD: 2.3717
Epoch 1 Training Complete - Avg Loss CE: 4.6089, Avg Loss KD: 2.3717

Epoch 1 Results - Train Acc: 8.12% | Test Acc: 10.71% | Best: 10.71%
0.01375809
Epoch 2: Training...

  Batch 100/782 - Loss CE: 3.5376, Loss KD: 4.3004
  Batch 200/782 - Loss CE: 3.5147, Loss KD: 4.2739
  Batch 300/782 - Loss CE: 3.4791, Loss KD: 4.2504
  Batch 400/782 - Loss CE: 3.4290, Loss KD: 4.2150
  Batch 500/782 - Loss CE: 3.3872, Loss KD: 4.1789
  Batch 600/782 - Loss CE: 3.3534, Loss KD: 4.1531
  Batch 700/782 - Loss CE: 3.3096, Loss KD: 4.1184
  Batch 782/782 - Loss CE: 3.2782, Loss KD: 4.0947
Epoch 2 Training Complete - Avg Loss CE: 3.2782, Avg Loss KD: 4.0947

Epoch 2 Results - Train Acc: 19.02% | Test Acc: 23.85% | Best: 23.85%
0.013522879166666666
Epoch 3: Training...

  Batch 100/782 - Loss CE: 3.0301, Loss KD: 5.8090
  Batch 200/782 - Loss CE: 2.9871, Loss KD: 5.7507
  Batch 300/782 - Loss CE: 2.9608, Loss KD: 5.7126
  Batch 400/782 - Loss CE: 2.9271, Loss KD: 5.6693
  Batch 500/782 - Loss CE: 2.9013, Loss KD: 5.6329
  Batch 600/782 - Loss CE: 2.8789, Loss KD: 5.5968
  Batch 700/782 - Loss CE: 2.8543, Loss KD: 5.5616
  Batch 782/782 - Loss CE: 2.8359, Loss KD: 5.5381
Epoch 3 Training Complete - Avg Loss CE: 2.8359, Avg Loss KD: 5.5381

Epoch 3 Results - Train Acc: 27.13% | Test Acc: 29.15% | Best: 29.15%
0.013503789444444443
Epoch 4: Training...

  Batch 100/782 - Loss CE: 2.6675, Loss KD: 7.0553
  Batch 200/782 - Loss CE: 2.6277, Loss KD: 7.0079
  Batch 300/782 - Loss CE: 2.6225, Loss KD: 6.9936
  Batch 400/782 - Loss CE: 2.6012, Loss KD: 6.9499
  Batch 500/782 - Loss CE: 2.5829, Loss KD: 6.9049
  Batch 600/782 - Loss CE: 2.5706, Loss KD: 6.8724
  Batch 700/782 - Loss CE: 2.5609, Loss KD: 6.8453
  Batch 782/782 - Loss CE: 2.5544, Loss KD: 6.8282
Epoch 4 Training Complete - Avg Loss CE: 2.5544, Avg Loss KD: 6.8282

Epoch 4 Results - Train Acc: 33.33% | Test Acc: 32.40% | Best: 32.40%
0.0136823425
Epoch 5: Training...

  Batch 100/782 - Loss CE: 2.3844, Loss KD: 8.1415
  Batch 200/782 - Loss CE: 2.4052, Loss KD: 8.1305
  Batch 300/782 - Loss CE: 2.4075, Loss KD: 8.1277
  Batch 400/782 - Loss CE: 2.3923, Loss KD: 8.0859
  Batch 500/782 - Loss CE: 2.3754, Loss KD: 8.0316
  Batch 600/782 - Loss CE: 2.3657, Loss KD: 7.9926
  Batch 700/782 - Loss CE: 2.3598, Loss KD: 7.9607
  Batch 782/782 - Loss CE: 2.3479, Loss KD: 7.9241
Epoch 5 Training Complete - Avg Loss CE: 2.3479, Avg Loss KD: 7.9241

Epoch 5 Results - Train Acc: 37.99% | Test Acc: 36.41% | Best: 36.41%
0.013488103333333334
Epoch 6: Training...

