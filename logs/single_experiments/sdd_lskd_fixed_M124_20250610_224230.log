<class 'str'>
[36m[INFO] CONFIG:
DATASET:
  NUM_WORKERS: 2
  TEST:
    BATCH_SIZE: 64
  TYPE: cifar100
DISTILLER:
  STUDENT: resnet8x4_sdd
  TEACHER: resnet32x4_sdd
  TYPE: SDD_LSKD
EXPERIMENT:
  NAME: ''
  PROJECT: cifar100_baselines
  TAG: sdd_lskd,res32x4_sdd,res8x4_sdd
LOG:
  PREFIX: ./output
  SAVE_CHECKPOINT_FREQ: 40
  TENSORBOARD_FREQ: 500
  WANDB: false
SOLVER:
  BATCH_SIZE: 64
  EPOCHS: 240
  LR: 0.05
  LR_DECAY_RATE: 0.1
  LR_DECAY_STAGES:
  - 150
  - 180
  - 210
  MOMENTUM: 0.9
  TRAINER: base
  TYPE: SGD
  WEIGHT_DECAY: 0.0005
[0m
Files already downloaded and verified
Files already downloaded and verified
[36m[INFO] Loading teacher model[0m
[1,2,4]
[1,2,4]
[36m[INFO] Extra parameters of SDD_LSKD: 0[0m[0m
Epoch 1: Training...
/root/repos/SDD-LSKD-Fusion/mdistiller/engine/utils.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(f, map_location="cpu")
/root/repos/SDD-LSKD-Fusion/mdistiller/distillers/SDD_LSKD.py:130: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  global_prediction_true_mask_repeat = torch.tensor(global_prediction_true_mask).repeat(out_t_multi.shape[2])
/root/repos/SDD-LSKD-Fusion/mdistiller/distillers/SDD_LSKD.py:131: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  global_prediction_false_mask_repeat = torch.tensor(global_prediction_false_mask).repeat(out_t_multi.shape[2])
  Batch 100/782 - Loss CE: 4.3391, Loss KD: 0.0228
  Batch 200/782 - Loss CE: 4.2028, Loss KD: 0.0218
  Batch 300/782 - Loss CE: 4.1053, Loss KD: 0.0213
  Batch 400/782 - Loss CE: 4.0361, Loss KD: 0.0210
  Batch 500/782 - Loss CE: 3.9803, Loss KD: 0.0207
  Batch 600/782 - Loss CE: 3.9317, Loss KD: 0.0205
  Batch 700/782 - Loss CE: 3.8870, Loss KD: 0.0203
  Batch 782/782 - Loss CE: 3.8490, Loss KD: 0.0202
Epoch 1 Training Complete - Avg Loss CE: 3.8490, Avg Loss KD: 0.0202

Epoch 1 Results - Train Acc: 10.83% | Test Acc: 15.52% | Best: 15.52%
0.0049944575
Epoch 2: Training...

  Batch 100/782 - Loss CE: 3.4606, Loss KD: 0.0379
  Batch 200/782 - Loss CE: 3.4222, Loss KD: 0.0378
  Batch 300/782 - Loss CE: 3.3874, Loss KD: 0.0375
  Batch 400/782 - Loss CE: 3.3382, Loss KD: 0.0372
  Batch 500/782 - Loss CE: 3.3026, Loss KD: 0.0369
  Batch 600/782 - Loss CE: 3.2709, Loss KD: 0.0367
  Batch 700/782 - Loss CE: 3.2399, Loss KD: 0.0365
  Batch 782/782 - Loss CE: 3.2118, Loss KD: 0.0364
Epoch 2 Training Complete - Avg Loss CE: 3.2118, Avg Loss KD: 0.0364

Epoch 2 Results - Train Acc: 21.10% | Test Acc: 25.07% | Best: 25.07%
0.004485055
Epoch 3: Training...

  Batch 100/782 - Loss CE: 2.9108, Loss KD: 0.0523
  Batch 200/782 - Loss CE: 2.8851, Loss KD: 0.0520
  Batch 300/782 - Loss CE: 2.8633, Loss KD: 0.0517
  Batch 400/782 - Loss CE: 2.8313, Loss KD: 0.0515
  Batch 500/782 - Loss CE: 2.8028, Loss KD: 0.0512
  Batch 600/782 - Loss CE: 2.7763, Loss KD: 0.0509
  Batch 700/782 - Loss CE: 2.7497, Loss KD: 0.0507
  Batch 782/782 - Loss CE: 2.7309, Loss KD: 0.0505
Epoch 3 Training Complete - Avg Loss CE: 2.7309, Avg Loss KD: 0.0505

Epoch 3 Results - Train Acc: 29.90% | Test Acc: 33.57% | Best: 33.57%
0.0053077686111111105
Epoch 4: Training...

  Batch 100/782 - Loss CE: 2.5208, Loss KD: 0.0650
  Batch 200/782 - Loss CE: 2.4772, Loss KD: 0.0646
  Batch 300/782 - Loss CE: 2.4565, Loss KD: 0.0645
  Batch 400/782 - Loss CE: 2.4464, Loss KD: 0.0644
  Batch 500/782 - Loss CE: 2.4354, Loss KD: 0.0642
  Batch 600/782 - Loss CE: 2.4190, Loss KD: 0.0640
  Batch 700/782 - Loss CE: 2.4061, Loss KD: 0.0638
  Batch 782/782 - Loss CE: 2.3898, Loss KD: 0.0635
Epoch 4 Training Complete - Avg Loss CE: 2.3898, Avg Loss KD: 0.0635

Epoch 4 Results - Train Acc: 37.17% | Test Acc: 37.50% | Best: 37.50%
0.0052407861111111115
Epoch 5: Training...

  Batch 100/782 - Loss CE: 2.2304, Loss KD: 0.0772
  Batch 200/782 - Loss CE: 2.2341, Loss KD: 0.0771
  Batch 300/782 - Loss CE: 2.2152, Loss KD: 0.0769
  Batch 400/782 - Loss CE: 2.1984, Loss KD: 0.0767
  Batch 500/782 - Loss CE: 2.1868, Loss KD: 0.0766
  Batch 600/782 - Loss CE: 2.1775, Loss KD: 0.0764
  Batch 700/782 - Loss CE: 2.1647, Loss KD: 0.0762
  Batch 782/782 - Loss CE: 2.1553, Loss KD: 0.0760
Epoch 5 Training Complete - Avg Loss CE: 2.1553, Avg Loss KD: 0.0760

Epoch 5 Results - Train Acc: 42.18% | Test Acc: 41.18% | Best: 41.18%
0.004731077222222222
Epoch 6: Training...

  Batch 100/782 - Loss CE: 1.9888, Loss KD: 0.0895
  Batch 200/782 - Loss CE: 2.0044, Loss KD: 0.0892
  Batch 300/782 - Loss CE: 2.0127, Loss KD: 0.0893
  Batch 400/782 - Loss CE: 2.0055, Loss KD: 0.0892
  Batch 500/782 - Loss CE: 2.0049, Loss KD: 0.0890
  Batch 600/782 - Loss CE: 2.0000, Loss KD: 0.0888
  Batch 700/782 - Loss CE: 1.9982, Loss KD: 0.0886
  Batch 782/782 - Loss CE: 1.9928, Loss KD: 0.0885
Epoch 6 Training Complete - Avg Loss CE: 1.9928, Avg Loss KD: 0.0885

Epoch 6 Results - Train Acc: 45.52% | Test Acc: 40.39% | Best: 41.18%
0.004237046111111111
Epoch 7: Training...

  Batch 100/782 - Loss CE: 1.8973, Loss KD: 0.1018
  Batch 200/782 - Loss CE: 1.9041, Loss KD: 0.1017
  Batch 300/782 - Loss CE: 1.8899, Loss KD: 0.1013
  Batch 400/782 - Loss CE: 1.8853, Loss KD: 0.1010
  Batch 500/782 - Loss CE: 1.8826, Loss KD: 0.1008
  Batch 600/782 - Loss CE: 1.8808, Loss KD: 0.1007
  Batch 700/782 - Loss CE: 1.8763, Loss KD: 0.1006
  Batch 782/782 - Loss CE: 1.8810, Loss KD: 0.1006
Epoch 7 Training Complete - Avg Loss CE: 1.8810, Avg Loss KD: 0.1006

Epoch 7 Results - Train Acc: 48.60% | Test Acc: 41.65% | Best: 41.65%
0.0051573425
Epoch 8: Training...

  Batch 100/782 - Loss CE: 1.7822, Loss KD: 0.1128
  Batch 200/782 - Loss CE: 1.7848, Loss KD: 0.1130
  Batch 300/782 - Loss CE: 1.7945, Loss KD: 0.1131
  Batch 400/782 - Loss CE: 1.7905, Loss KD: 0.1129
  Batch 500/782 - Loss CE: 1.7887, Loss KD: 0.1127
  Batch 600/782 - Loss CE: 1.7787, Loss KD: 0.1124
  Batch 700/782 - Loss CE: 1.7756, Loss KD: 0.1123
  Batch 782/782 - Loss CE: 1.7735, Loss KD: 0.1122
Epoch 8 Training Complete - Avg Loss CE: 1.7735, Avg Loss KD: 0.1122

Epoch 8 Results - Train Acc: 50.98% | Test Acc: 43.16% | Best: 43.16%
0.005478766944444445
Epoch 9: Training...

  Batch 100/782 - Loss CE: 1.6994, Loss KD: 0.1244
  Batch 200/782 - Loss CE: 1.7147, Loss KD: 0.1248
  Batch 300/782 - Loss CE: 1.7122, Loss KD: 0.1243
  Batch 400/782 - Loss CE: 1.7107, Loss KD: 0.1241
  Batch 500/782 - Loss CE: 1.7096, Loss KD: 0.1240
  Batch 600/782 - Loss CE: 1.7127, Loss KD: 0.1241
  Batch 700/782 - Loss CE: 1.7109, Loss KD: 0.1241
  Batch 782/782 - Loss CE: 1.7103, Loss KD: 0.1240
Epoch 9 Training Complete - Avg Loss CE: 1.7103, Avg Loss KD: 0.1240

Epoch 9 Results - Train Acc: 52.83% | Test Acc: 46.18% | Best: 46.18%
0.004552156944444445
Epoch 10: Training...

  Batch 100/782 - Loss CE: 1.6338, Loss KD: 0.1364
  Batch 200/782 - Loss CE: 1.6503, Loss KD: 0.1363
  Batch 300/782 - Loss CE: 1.6353, Loss KD: 0.1361
  Batch 400/782 - Loss CE: 1.6412, Loss KD: 0.1361
  Batch 500/782 - Loss CE: 1.6510, Loss KD: 0.1361
  Batch 600/782 - Loss CE: 1.6507, Loss KD: 0.1360
  Batch 700/782 - Loss CE: 1.6514, Loss KD: 0.1358
  Batch 782/782 - Loss CE: 1.6497, Loss KD: 0.1357
Epoch 10 Training Complete - Avg Loss CE: 1.6497, Avg Loss KD: 0.1357

Epoch 10 Results - Train Acc: 54.45% | Test Acc: 46.53% | Best: 46.53%
0.004975980833333333
Epoch 11: Training...

  Batch 100/782 - Loss CE: 1.6108, Loss KD: 0.1483
  Batch 200/782 - Loss CE: 1.6012, Loss KD: 0.1481
  Batch 300/782 - Loss CE: 1.5995, Loss KD: 0.1479
  Batch 400/782 - Loss CE: 1.6046, Loss KD: 0.1477
  Batch 500/782 - Loss CE: 1.6053, Loss KD: 0.1477
  Batch 600/782 - Loss CE: 1.6051, Loss KD: 0.1474
  Batch 700/782 - Loss CE: 1.6012, Loss KD: 0.1473
  Batch 782/782 - Loss CE: 1.5997, Loss KD: 0.1471
Epoch 11 Training Complete - Avg Loss CE: 1.5997, Avg Loss KD: 0.1471

Epoch 11 Results - Train Acc: 55.52% | Test Acc: 50.29% | Best: 50.29%
0.004477335277777778
Epoch 12: Training...

  Batch 100/782 - Loss CE: 1.5303, Loss KD: 0.1592
  Batch 200/782 - Loss CE: 1.5596, Loss KD: 0.1597
  Batch 300/782 - Loss CE: 1.5552, Loss KD: 0.1594
  Batch 400/782 - Loss CE: 1.5560, Loss KD: 0.1592
  Batch 500/782 - Loss CE: 1.5580, Loss KD: 0.1589
  Batch 600/782 - Loss CE: 1.5508, Loss KD: 0.1586
  Batch 700/782 - Loss CE: 1.5524, Loss KD: 0.1584
  Batch 782/782 - Loss CE: 1.5551, Loss KD: 0.1583
Epoch 12 Training Complete - Avg Loss CE: 1.5551, Avg Loss KD: 0.1583

Epoch 12 Results - Train Acc: 56.54% | Test Acc: 50.24% | Best: 50.29%
0.004852940277777777
Epoch 13: Training...

  Batch 100/782 - Loss CE: 1.5082, Loss KD: 0.1702
  Batch 200/782 - Loss CE: 1.5322, Loss KD: 0.1709
  Batch 300/782 - Loss CE: 1.5316, Loss KD: 0.1706
  Batch 400/782 - Loss CE: 1.5281, Loss KD: 0.1703
  Batch 500/782 - Loss CE: 1.5242, Loss KD: 0.1702
  Batch 600/782 - Loss CE: 1.5303, Loss KD: 0.1701
  Batch 700/782 - Loss CE: 1.5307, Loss KD: 0.1698
  Batch 782/782 - Loss CE: 1.5284, Loss KD: 0.1697
Epoch 13 Training Complete - Avg Loss CE: 1.5284, Avg Loss KD: 0.1697

Epoch 13 Results - Train Acc: 57.40% | Test Acc: 49.05% | Best: 50.29%
0.003953041666666666
Epoch 14: Training...

  Batch 100/782 - Loss CE: 1.5083, Loss KD: 0.1817
  Batch 200/782 - Loss CE: 1.4944, Loss KD: 0.1819
  Batch 300/782 - Loss CE: 1.4833, Loss KD: 0.1814
  Batch 400/782 - Loss CE: 1.4888, Loss KD: 0.1814
  Batch 500/782 - Loss CE: 1.4872, Loss KD: 0.1814
  Batch 600/782 - Loss CE: 1.4900, Loss KD: 0.1811
  Batch 700/782 - Loss CE: 1.4968, Loss KD: 0.1812
  Batch 782/782 - Loss CE: 1.5006, Loss KD: 0.1811
Epoch 14 Training Complete - Avg Loss CE: 1.5006, Avg Loss KD: 0.1811

Epoch 14 Results - Train Acc: 57.88% | Test Acc: 46.28% | Best: 50.29%
0.005007505277777778
Epoch 15: Training...

  Batch 100/782 - Loss CE: 1.4832, Loss KD: 0.1934
  Batch 200/782 - Loss CE: 1.4617, Loss KD: 0.1931
  Batch 300/782 - Loss CE: 1.4698, Loss KD: 0.1929
  Batch 400/782 - Loss CE: 1.4626, Loss KD: 0.1923
  Batch 500/782 - Loss CE: 1.4675, Loss KD: 0.1922
  Batch 600/782 - Loss CE: 1.4720, Loss KD: 0.1922
  Batch 700/782 - Loss CE: 1.4743, Loss KD: 0.1921
  Batch 782/782 - Loss CE: 1.4765, Loss KD: 0.1920
Epoch 15 Training Complete - Avg Loss CE: 1.4765, Avg Loss KD: 0.1920

Epoch 15 Results - Train Acc: 58.70% | Test Acc: 49.78% | Best: 50.29%
0.004822139722222222
Epoch 16: Training...

  Batch 100/782 - Loss CE: 1.4019, Loss KD: 0.2031
  Batch 200/782 - Loss CE: 1.4021, Loss KD: 0.2033
  Batch 300/782 - Loss CE: 1.4200, Loss KD: 0.2040
  Batch 400/782 - Loss CE: 1.4331, Loss KD: 0.2039
  Batch 500/782 - Loss CE: 1.4443, Loss KD: 0.2036
  Batch 600/782 - Loss CE: 1.4489, Loss KD: 0.2034
  Batch 700/782 - Loss CE: 1.4547, Loss KD: 0.2033
  Batch 782/782 - Loss CE: 1.4578, Loss KD: 0.2033
Epoch 16 Training Complete - Avg Loss CE: 1.4578, Avg Loss KD: 0.2033

Epoch 16 Results - Train Acc: 59.11% | Test Acc: 48.41% | Best: 50.29%
0.005561947777777778
Epoch 17: Training...

  Batch 100/782 - Loss CE: 1.3794, Loss KD: 0.2143
  Batch 200/782 - Loss CE: 1.3897, Loss KD: 0.2146
  Batch 300/782 - Loss CE: 1.4066, Loss KD: 0.2150
  Batch 400/782 - Loss CE: 1.4233, Loss KD: 0.2151
  Batch 500/782 - Loss CE: 1.4282, Loss KD: 0.2149
  Batch 600/782 - Loss CE: 1.4310, Loss KD: 0.2149
  Batch 700/782 - Loss CE: 1.4301, Loss KD: 0.2145
  Batch 782/782 - Loss CE: 1.4333, Loss KD: 0.2146
Epoch 17 Training Complete - Avg Loss CE: 1.4333, Avg Loss KD: 0.2146

Epoch 17 Results - Train Acc: 59.70% | Test Acc: 48.29% | Best: 50.29%
0.005346618055555555
Epoch 18: Training...

  Batch 100/782 - Loss CE: 1.3930, Loss KD: 0.2258
  Batch 200/782 - Loss CE: 1.3793, Loss KD: 0.2253
  Batch 300/782 - Loss CE: 1.3943, Loss KD: 0.2257
  Batch 400/782 - Loss CE: 1.4027, Loss KD: 0.2258
  Batch 500/782 - Loss CE: 1.4090, Loss KD: 0.2256
  Batch 600/782 - Loss CE: 1.4148, Loss KD: 0.2256
  Batch 700/782 - Loss CE: 1.4170, Loss KD: 0.2255
  Batch 782/782 - Loss CE: 1.4173, Loss KD: 0.2253
Epoch 18 Training Complete - Avg Loss CE: 1.4173, Avg Loss KD: 0.2253

Epoch 18 Results - Train Acc: 60.15% | Test Acc: 53.24% | Best: 53.24%
0.0048731669444444445
Epoch 19: Training...

  Batch 100/782 - Loss CE: 1.3749, Loss KD: 0.2371
  Batch 200/782 - Loss CE: 1.3830, Loss KD: 0.2370
  Batch 300/782 - Loss CE: 1.3785, Loss KD: 0.2370
  Batch 400/782 - Loss CE: 1.3824, Loss KD: 0.2367
  Batch 500/782 - Loss CE: 1.3978, Loss KD: 0.2366
  Batch 600/782 - Loss CE: 1.3916, Loss KD: 0.2360
  Batch 700/782 - Loss CE: 1.3972, Loss KD: 0.2361
  Batch 782/782 - Loss CE: 1.3991, Loss KD: 0.2360
Epoch 19 Training Complete - Avg Loss CE: 1.3991, Avg Loss KD: 0.2360

Epoch 19 Results - Train Acc: 60.62% | Test Acc: 52.29% | Best: 53.24%
0.005267100555555556
Epoch 20: Training...

  Batch 100/782 - Loss CE: 1.3238, Loss KD: 0.2450
  Batch 200/782 - Loss CE: 1.3530, Loss KD: 0.2465
  Batch 300/782 - Loss CE: 1.3626, Loss KD: 0.2469
  Batch 400/782 - Loss CE: 1.3670, Loss KD: 0.2469
  Batch 500/782 - Loss CE: 1.3751, Loss KD: 0.2473
  Batch 600/782 - Loss CE: 1.3785, Loss KD: 0.2473
  Batch 700/782 - Loss CE: 1.3852, Loss KD: 0.2472
  Batch 782/782 - Loss CE: 1.3888, Loss KD: 0.2472
Epoch 20 Training Complete - Avg Loss CE: 1.3888, Avg Loss KD: 0.2472

Epoch 20 Results - Train Acc: 60.89% | Test Acc: 53.69% | Best: 53.69%
0.0052102749999999995
Epoch 21: Training...

  Batch 100/782 - Loss CE: 1.3533, Loss KD: 0.2449
  Batch 200/782 - Loss CE: 1.3500, Loss KD: 0.2450
  Batch 300/782 - Loss CE: 1.3557, Loss KD: 0.2453
  Batch 400/782 - Loss CE: 1.3722, Loss KD: 0.2458
  Batch 500/782 - Loss CE: 1.3759, Loss KD: 0.2456
  Batch 600/782 - Loss CE: 1.3854, Loss KD: 0.2460
  Batch 700/782 - Loss CE: 1.3860, Loss KD: 0.2460
  Batch 782/782 - Loss CE: 1.3844, Loss KD: 0.2457
Epoch 21 Training Complete - Avg Loss CE: 1.3844, Avg Loss KD: 0.2457

Epoch 21 Results - Train Acc: 60.82% | Test Acc: 52.34% | Best: 53.69%
0.005532891944444444
Epoch 22: Training...

  Batch 100/782 - Loss CE: 1.3081, Loss KD: 0.2433
  Batch 200/782 - Loss CE: 1.3218, Loss KD: 0.2440
  Batch 300/782 - Loss CE: 1.3375, Loss KD: 0.2446
  Batch 400/782 - Loss CE: 1.3524, Loss KD: 0.2451
  Batch 500/782 - Loss CE: 1.3590, Loss KD: 0.2451
  Batch 600/782 - Loss CE: 1.3608, Loss KD: 0.2450
  Batch 700/782 - Loss CE: 1.3647, Loss KD: 0.2450
  Batch 782/782 - Loss CE: 1.3668, Loss KD: 0.2450
Epoch 22 Training Complete - Avg Loss CE: 1.3668, Avg Loss KD: 0.2450

Epoch 22 Results - Train Acc: 61.22% | Test Acc: 53.50% | Best: 53.69%
0.005638564166666667
Epoch 23: Training...

  Batch 100/782 - Loss CE: 1.3094, Loss KD: 0.2432
  Batch 200/782 - Loss CE: 1.3152, Loss KD: 0.2428
  Batch 300/782 - Loss CE: 1.3224, Loss KD: 0.2431
  Batch 400/782 - Loss CE: 1.3322, Loss KD: 0.2434
  Batch 500/782 - Loss CE: 1.3384, Loss KD: 0.2436
  Batch 600/782 - Loss CE: 1.3414, Loss KD: 0.2435
  Batch 700/782 - Loss CE: 1.3475, Loss KD: 0.2437
  Batch 782/782 - Loss CE: 1.3502, Loss KD: 0.2439
Epoch 23 Training Complete - Avg Loss CE: 1.3502, Avg Loss KD: 0.2439

Epoch 23 Results - Train Acc: 61.78% | Test Acc: 52.44% | Best: 53.69%
0.005219855833333334
Epoch 24: Training...

  Batch 100/782 - Loss CE: 1.3034, Loss KD: 0.2436
  Batch 200/782 - Loss CE: 1.3184, Loss KD: 0.2441
  Batch 300/782 - Loss CE: 1.3259, Loss KD: 0.2440
  Batch 400/782 - Loss CE: 1.3322, Loss KD: 0.2438
  Batch 500/782 - Loss CE: 1.3366, Loss KD: 0.2437
  Batch 600/782 - Loss CE: 1.3432, Loss KD: 0.2439
  Batch 700/782 - Loss CE: 1.3486, Loss KD: 0.2441
  Batch 782/782 - Loss CE: 1.3538, Loss KD: 0.2442
Epoch 24 Training Complete - Avg Loss CE: 1.3538, Avg Loss KD: 0.2442

Epoch 24 Results - Train Acc: 61.61% | Test Acc: 51.55% | Best: 53.69%
0.00507904888888889
Epoch 25: Training...

  Batch 100/782 - Loss CE: 1.2843, Loss KD: 0.2422
  Batch 200/782 - Loss CE: 1.2989, Loss KD: 0.2423
  Batch 300/782 - Loss CE: 1.3077, Loss KD: 0.2427
  Batch 400/782 - Loss CE: 1.3174, Loss KD: 0.2435
  Batch 500/782 - Loss CE: 1.3222, Loss KD: 0.2430
  Batch 600/782 - Loss CE: 1.3300, Loss KD: 0.2431
  Batch 700/782 - Loss CE: 1.3362, Loss KD: 0.2434
  Batch 782/782 - Loss CE: 1.3430, Loss KD: 0.2434
Epoch 25 Training Complete - Avg Loss CE: 1.3430, Avg Loss KD: 0.2434

Epoch 25 Results - Train Acc: 61.92% | Test Acc: 54.93% | Best: 54.93%
0.0053767569444444436
Epoch 26: Training...

  Batch 100/782 - Loss CE: 1.2664, Loss KD: 0.2416
  Batch 200/782 - Loss CE: 1.2950, Loss KD: 0.2412
  Batch 300/782 - Loss CE: 1.2839, Loss KD: 0.2405
  Batch 400/782 - Loss CE: 1.3043, Loss KD: 0.2413
  Batch 500/782 - Loss CE: 1.3121, Loss KD: 0.2416
  Batch 600/782 - Loss CE: 1.3201, Loss KD: 0.2420
  Batch 700/782 - Loss CE: 1.3237, Loss KD: 0.2422
  Batch 782/782 - Loss CE: 1.3314, Loss KD: 0.2424
Epoch 26 Training Complete - Avg Loss CE: 1.3314, Avg Loss KD: 0.2424

Epoch 26 Results - Train Acc: 62.38% | Test Acc: 50.72% | Best: 54.93%
0.005030926388888889
Epoch 27: Training...

EXPERIMENT_FAILED_OR_TIMEOUT
